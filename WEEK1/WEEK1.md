# WEEK 1

## Summarization
I started this journey for keeping the logs of what I read through this entire journey and turning it into a challenge for completing in 6 weeks. This was the Week 1 and I started by reading the *"Unreasonable Effects of RNN"*. I didn't know much things about RNN and NLP until this paper. So it was a starting point for me. I really liked how the RNNs turn the recursivity into a computational beast. Having the flexibility of using the sequence based architechture was amazing. 

Soon after that, I read the *"Understanding LSTM Networks"* because I heard that the LSTM are more populer than the native RNN because of the ability of memory. I understood how it differs from the RNNs mostly and how it keeps the track of what was important.

Then after a little break for school lectures, I read a paper which is *"Attention and Augmented Recurrent Neural Networks"* that slightly mentions the few usages of attention mechanism. The overall design and the interactivity of the blog was incredible. I recommend for anyone who wants a peek behind the curtain.

Lastly, I read the *"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"* paper. But I can assure you I didn't get most of the paper. It has a complicated mathematical language and there are a lot of prerequisites so I will turn back to this one after maybe Week 3 or Week 4. However I got some idea about how attention mechanism affects a model and the difference between "soft" and "hard" attention.

**What I'll do next:** In Week 2, I want to read a little more about the overall RNN and LSTMs then jump right into the constructing and training a native RNN and a LSTM model. I think it'll be the best for me to do some coding or else I don't think I can dive deeper.

I've also included my handwritten notes, which I took to stay focused while reading rather than for revision.

## Chronological Order
1. <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">"The Unreasonable Effectiveness of Recurrent Neural Networks"</a>
2. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">"Understanding LSTM Networks"</a>
3. <a href="https://distill.pub/2016/augmented-rnns/">"Attention and Augmented Recurrent Neural Networks"</a>
4. <a href="https://arxiv.org/pdf/1502.03044v2">"Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention"</a>
